{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1926b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T04:04:03.798166Z",
     "iopub.status.busy": "2022-02-23T04:04:03.793692Z",
     "iopub.status.idle": "2022-02-23T04:04:07.528119Z",
     "shell.execute_reply": "2022-02-23T04:04:07.527579Z",
     "shell.execute_reply.started": "2022-02-23T03:59:29.356962Z"
    },
    "papermill": {
     "duration": 3.75644,
     "end_time": "2022-02-23T04:04:07.528248",
     "exception": false,
     "start_time": "2022-02-23T04:04:03.771808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from preprocess import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb90816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d9eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb1811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleDataProcessor(DataProcessor):\n",
    "    \"\"\"Data processor for rule predictor\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir:str):\n",
    "        super(RuleDataProcessor, self).__init__(data_dir)\n",
    "    \n",
    "    def split_data(self, trans_data:pd.DataFrame, train_end_date:str, valid_end_date:str) -> Tuple[pd.DataFrame]:\n",
    "        \"\"\"Split transaction data into train set and valid set\n",
    "\n",
    "        Args:\n",
    "            trans_data (pd.DataFrame): transaction dataframe\n",
    "            train_end_date (str): end date of train set, max(train_set.date) <= train_end_date\n",
    "            valid_end_date (str): end date of valid set, max(valid_set.date) <= valid_end_date\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame]: [train set, valid_set]\n",
    "        \"\"\"\n",
    "        train_set = trans_data.loc[trans_data['t_dat']<=train_end_date]\n",
    "        valid_set = trans_data.loc[(train_end_date<trans_data['t_dat'])&(trans_data['t_dat']<=valid_end_date)]\n",
    "        valid_set = valid_set.groupby(['customer_id'])['article_id'].apply(list).reset_index()\n",
    "\n",
    "        return train_set, valid_set\n",
    "    \n",
    "    def history_purchase(self, train_set:pd.DataFrame, days:int=7) -> pd.DataFrame:\n",
    "        \"\"\"Filter history purchase items in the last n days\n",
    "\n",
    "        Args:\n",
    "            train_set (pd.DataFrame): transaction dataframe\n",
    "            days (int, optional): length of history\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: filtered dataframe\n",
    "        \"\"\"\n",
    "        train_set['t_dat'] = pd.to_datetime(train_set['t_dat'])\n",
    "        tmp = train_set.groupby('customer_id').t_dat.max().reset_index()\n",
    "        tmp.columns = ['customer_id','max_dat']\n",
    "        res = train_set[['customer_id','article_id','t_dat']].merge(tmp,on=['customer_id'],how='left')\n",
    "        res['diff_dat'] = (res.max_dat - res.t_dat).dt.days\n",
    "        res = res.loc[res['diff_dat']<days].reset_index(drop=True)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def get_freq_pair(self, train_set:pd.DataFrame, n:int=3) -> dict:\n",
    "        \"\"\"Generate dict of frequent item pairs in target time window\n",
    "\n",
    "        Args:\n",
    "            train_set (pd.DataFrame): transaction dataframe in target time window\n",
    "            n (int, optional): search top *n* pairs for each article. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            dict: frequent item pairs\n",
    "        \"\"\"\n",
    "        tmp = train_set.drop_duplicates(['customer_id','article_id'])\n",
    "        s = tmp[['customer_id','article_id']].merge(tmp[['customer_id','article_id']],on='customer_id')\n",
    "        s = s[s['article_id_x']!=s['article_id_y']]\n",
    "        s['count'] = 1\n",
    "        s = s.groupby(['article_id_x','article_id_y'],as_index=False)['count'].sum()\n",
    "        s = s.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "        s = s.groupby('article_id_x')['article_id_y'].apply(lambda x:list(x)[:n])\n",
    "        s = s.to_dict()\n",
    "\n",
    "        res = {}\n",
    "        for k in s.keys():\n",
    "            for i in s[k]:\n",
    "                res[k] = i\n",
    "        return res\n",
    "\n",
    "    def popular_item(self, train_set:pd.DataFrame, n:int=20) -> List[int]:\n",
    "        \"\"\"Generate list of popular items in target time windows\n",
    "\n",
    "        Args:\n",
    "            train_set (pd.DataFrame): transaction dataframe in target time window\n",
    "            n (int, optional): return top *n* popular items. Defaults to 20.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: popular items\n",
    "        \"\"\"\n",
    "        tmp = train_set.drop_duplicates(['customer_id','article_id'])\n",
    "        tmp['count'] = 1\n",
    "        tmp = tmp.groupby(['article_id'],as_index=False)['count'].sum()\n",
    "        tmp = tmp.sort_values(by='count',ascending=False)\n",
    "        \n",
    "        return tmp['article_id'].values[:n]\n",
    "    \n",
    "    def off_item(self, trans_data:pd.DataFrame) -> List[int]:\n",
    "        \"\"\"Find items that seem to be off stock\n",
    "\n",
    "        Args:\n",
    "            trans_data (pd.DataFrame): transaction_data\n",
    "\n",
    "        Returns:\n",
    "            List[int]: off stock items\n",
    "        \"\"\"\n",
    "        item_sale = trans_data.copy()\n",
    "        item_sale['t_dat'] = pd.to_datetime(item_sale['t_dat'])\n",
    "        item_sale['year_month'] = (item_sale['t_dat'].dt.year).astype(str) + '_' + (item_sale['t_dat'].dt.month).astype(str)\n",
    "        item_sale = item_sale.groupby(['article_id','year_month'])['customer_id'].agg('count').reset_index()\n",
    "        item_sale.rename(columns={'customer_id':'count'}, inplace=True)\n",
    "\n",
    "        item_sale = pd.pivot_table(item_sale, values='count', index='article_id', columns='year_month')\n",
    "        item_sale = item_sale.fillna(0)\n",
    "        mask = ((item_sale['2020_9'] - item_sale['2020_8']) / item_sale['2020_8']) < -0.8\n",
    "        mask2 = item_sale['2020_9'] == 0\n",
    "\n",
    "        return list(item_sale[mask | mask2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78fc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = RuleDataProcessor('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18d05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = dp.preprocess_data(save=True, name='encoded_full')\n",
    "data = dp.load_data('encoded_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = data['trans']\n",
    "user = data['user']\n",
    "item = data['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8ffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['trans'] = trans\n",
    "data['item'] = item\n",
    "data['user'] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff3575",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5bae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = dp.split_data(trans, '2020-09-15', '2020-09-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f75a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weipeng Zhang\\AppData\\Local\\Temp\\ipykernel_29104\\1954538893.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set['t_dat'] = pd.to_datetime(train_set['t_dat'])\n"
     ]
    }
   ],
   "source": [
    "last_week = dp.history_purchase(train, days=7)\n",
    "sept = train[pd.to_datetime(train['t_dat']).dt.month==9]\n",
    "\n",
    "pairs1 = dp.get_freq_pair(last_week, n=1)\n",
    "pairs2 = dp.get_freq_pair(sept, n=1)\n",
    "\n",
    "last_week['article_id2'] = last_week['article_id'].map(pairs1)\n",
    "sept['article_id2'] = sept['article_id'].map(pairs1)\n",
    "\n",
    "train2 = last_week[['customer_id','article_id2']].copy()\n",
    "train2 = train2.loc[train2.article_id2.notnull()]\n",
    "train2 = train2.drop_duplicates(['customer_id','article_id2'])\n",
    "train2 = train2.rename({'article_id2':'article_id'},axis=1)\n",
    "train2['article_id'] = train2['article_id'].astype('int32')\n",
    "\n",
    "train3 = sept[['customer_id','article_id2']].copy()\n",
    "train3 = train3.loc[train3.article_id2.notnull()]\n",
    "train3 = train3.drop_duplicates(['customer_id','article_id2'])\n",
    "train3 = train3.rename({'article_id2':'article_id'},axis=1)\n",
    "train3['article_id'] = train3['article_id'].astype('int32')\n",
    "\n",
    "pred = pd.concat([last_week[['customer_id','article_id']], train2, sept[['customer_id','article_id']], train3])\n",
    "pred = pred.drop_duplicates(['customer_id','article_id'])\n",
    "\n",
    "off_stock_items = dp.off_item(trans)\n",
    "pred = pred[~pred['article_id'].isin(off_stock_items)]\n",
    "pred['article_id'] = ' '+ pred['article_id'].astype(str) + ' '\n",
    "pred = pred.groupby('customer_id')['article_id'].sum().reset_index()\n",
    "pred.columns = ['customer_id','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "top12 = dp.popular_item(trans[trans['t_dat']>'2020-09-15'], n=12)\n",
    "valid = pd.merge(valid, pred, on=['customer_id'], how='left')\n",
    "valid['prediction'] = valid['prediction'].fillna('')\n",
    "valid['prediction'] += ' '.join([str(x) for x in top12])\n",
    "valid['prediction'] = valid['prediction'].apply(lambda x:[int(i) for i in x.split()[:12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted) if a]) # CHANGES: ignore null actual (variable=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023569954298159303"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(valid['article_id'], valid['prediction'], k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02245225856110233"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(valid['article_id'], valid['prediction'], k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021503000716863493"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(valid['article_id'], valid['prediction'], k=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c86c4",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989b9b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weipeng Zhang\\AppData\\Local\\Temp\\ipykernel_13952\\2652833411.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sept['article_id2'] = sept['article_id'].map(pairs1)\n"
     ]
    }
   ],
   "source": [
    "last_week = dp.history_purchase(trans, days=7)\n",
    "sept = trans[pd.to_datetime(trans['t_dat']).dt.month==9]\n",
    "\n",
    "pairs1 = dp.get_freq_pair(last_week, n=1)\n",
    "pairs2 = dp.get_freq_pair(sept, n=1)\n",
    "\n",
    "last_week['article_id2'] = last_week['article_id'].map(pairs1)\n",
    "sept['article_id2'] = sept['article_id'].map(pairs1)\n",
    "\n",
    "train2 = last_week[['customer_id','article_id2']].copy()\n",
    "train2 = train2.loc[train2.article_id2.notnull()]\n",
    "train2 = train2.drop_duplicates(['customer_id','article_id2'])\n",
    "train2 = train2.rename({'article_id2':'article_id'},axis=1)\n",
    "train2['article_id'] = train2['article_id'].astype('int32')\n",
    "\n",
    "train3 = sept[['customer_id','article_id2']].copy()\n",
    "train3 = train3.loc[train3.article_id2.notnull()]\n",
    "train3 = train3.drop_duplicates(['customer_id','article_id2'])\n",
    "train3 = train3.rename({'article_id2':'article_id'},axis=1)\n",
    "train3['article_id'] = train3['article_id'].astype('int32')\n",
    "\n",
    "pred = pd.concat([last_week[['customer_id','article_id']], train2, sept[['customer_id','article_id']], train3])\n",
    "pred = pred.drop_duplicates(['customer_id','article_id'])\n",
    "\n",
    "off_stock_items = dp.off_item(trans)\n",
    "pred = pred[~pred['article_id'].isin(off_stock_items)]\n",
    "pred['article_id'] = ' '+ pred['article_id'].astype(str) + ' '\n",
    "pred = pred.groupby('customer_id')['article_id'].sum().reset_index()\n",
    "pred.columns = ['customer_id','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee6cfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348f5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96873147",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_path = dp.base + 'index_id_map/'\n",
    "\n",
    "user_index2id_dict = pickle.load(open(map_path+'/user_index2id.pkl','rb'))\n",
    "item_index2id_dict = pickle.load(open(map_path+'/item_index2id.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4825ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['customer_id'] = pred['customer_id'].map(user_index2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6372a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weipeng Zhang\\AppData\\Local\\Temp\\ipykernel_13952\\1954538893.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "del sub['prediction']\n",
    "top12 = dp.popular_item(trans[trans['t_dat']>'2020-09-15'], n=12)\n",
    "sub = pd.merge(sub, pred, on=['customer_id'], how='left')\n",
    "sub['prediction'] = sub['prediction'].fillna('')\n",
    "sub['prediction'] += ' '.join([str(x) for x in top12])\n",
    "sub['prediction'] = sub['prediction'].apply(lambda x:[int(i) for i in x.split()[:12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8851280",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['prediction'] = sub['prediction'].apply(lambda x:' '.join(['0'+str(item_index2id_dict[i]) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf36d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 78.544809,
   "end_time": "2022-02-23T04:05:14.265925",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-23T04:03:55.721116",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

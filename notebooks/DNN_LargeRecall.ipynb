{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt9B5f7NRIda",
        "outputId": "4596f5ae-8aa6-40cf-e27a-8c380b64c354"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEw9Y3BMiZkY",
        "outputId": "973e0637-dbde-40ad-8601-dedda916eb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_EiplKjw4dok"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/HM-new/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kXPJ-c7F4fja"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, BatchNormalization, Concatenate, Activation\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "_UiH-5IQRMQa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dXrnVUXj7tT3"
      },
      "outputs": [],
      "source": [
        "from src.data import DataHelper\n",
        "from src.data.metrics import map_at_k, recall_at_k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "v9Qx0k0ZKRup"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tJxGhYB45S5V"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# * uncomment this when predicting testset to avoid GPU memory error\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
      ],
      "metadata": {
        "id": "f2qZdUFqgtPF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5mjYRXj54tY1"
      },
      "outputs": [],
      "source": [
        "RANK_EMBEDDING_DIM = 64\n",
        "BATCH_SIZE = 2**12\n",
        "NEPOCH = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ImqKrQio47V1"
      },
      "outputs": [],
      "source": [
        "TRAIN_WEEK_NUM = 4\n",
        "WEEK_NUM = TRAIN_WEEK_NUM + 2\n",
        "\n",
        "VERSION_NAME = \"LargeRecall\"\n",
        "TEST = False # * Set as `False` when do local experiments to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MS0MEj1z5Rxs"
      },
      "outputs": [],
      "source": [
        "data_dir = Path(\"/content/drive/MyDrive/HM-new/data/\")\n",
        "model_dir = Path(\"/content/drive/MyDrive/HM-new/models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tbasc4Yn7vjP"
      },
      "outputs": [],
      "source": [
        "dh = DataHelper(data_dir)\n",
        "data = dh.load_data(name=\"encoded_full\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inter = data['inter']\n",
        "inter = inter.loc[(inter.t_dat <= \"2020-08-19\")]"
      ],
      "metadata": {
        "id": "Dvqsn-x2NSVR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate & Load Embeddings"
      ],
      "metadata": {
        "id": "ZvLo7j94F0l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# article description - TFIDF - SVD\n",
        "if not os.path.exists(data_dir/'external/tfidf_item_embd.npy'):\n",
        "    articles = pd.read_csv(data_dir/'raw/articles.csv')\n",
        "\n",
        "    corpus = articles[[col for col in articles.columns if 'name' in col] + ['detail_desc']].T.apply(lambda x: ' '.join(map(str,x))).T\n",
        "\n",
        "    vectorizer = TfidfVectorizer(min_df=3)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    svd = TruncatedSVD(n_components=256, random_state=0)\n",
        "    tfidf_item = svd.fit_transform(X)\n",
        "    tfidf_item = np.concatenate([np.ones((1,256)), tfidf_item], axis=0)\n",
        "    tfidf_item.dump(data_dir/'external/tfidf_item_embd.npy')\n",
        "else:\n",
        "    tfidf_item = np.load(data_dir/'external/tfidf_item_embd.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "p-dWWWZ8F5Pk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# article_id - customer_id TFIDF + SVD\n",
        "if not os.path.exists(data_dir/'external/tfidf_item_embd2.npy'):\n",
        "    corpus = inter.groupby('article_id').customer_id.apply(lambda x: ' '.join(map(str, x)))\n",
        "    article_ids = np.array(list(corpus.index))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(min_df=3)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    svd = TruncatedSVD(n_components=128, random_state=0)\n",
        "    X_svd = svd.fit_transform(X)\n",
        "\n",
        "    item_num = data['item']['article_id'].nunique()\n",
        "    tfidf_item2 = np.ones((item_num+1, 128)) / 128\n",
        "    for i,iid in enumerate(article_ids):\n",
        "        tfidf_item2[iid,:] = X_svd[i,:]\n",
        "\n",
        "    tfidf_item2.dump(data_dir/'external/tfidf_item_embd2.npy')\n",
        "else:\n",
        "    tfidf_item2 = np.load(data_dir/'external/tfidf_item_embd2.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "-36br0IjNpM7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customer_id - product_code TFIDF + SVD\n",
        "if not os.path.exists(data_dir/'external/tfidf_user_embd.npy'):\n",
        "    inter = inter.merge(data['item'][['article_id','product_code']], on=['article_id'], how='left')\n",
        "    corpus = inter.groupby('customer_id').product_code.apply(lambda x: ' '.join(map(str, x)))\n",
        "    customer_ids = np.array(list(corpus.index))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(min_df=3)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    svd = TruncatedSVD(n_components=128, random_state=0)\n",
        "    X_svd = svd.fit_transform(X)\n",
        "\n",
        "    user_num = data['user']['customer_id'].nunique()\n",
        "    tfidf_user = np.ones((user_num+1, 128)) / 128\n",
        "    for i,uid in enumerate(customer_ids):\n",
        "        tfidf_user[uid,:] = X_svd[i,:]\n",
        "\n",
        "    tfidf_user.dump(data_dir/'external/tfidf_user_embd.npy')\n",
        "else:\n",
        "    tfidf_user = np.load(data_dir/'external/tfidf_user_embd.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "QF8DZs-wMkUl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# * Load pre-trained embeddings\n",
        "w2v_user_embd = np.load(data_dir/'external'/'w2v_user_embd.npy', allow_pickle=True)\n",
        "w2v_item_embd = np.load(data_dir/'external'/'w2v_item_embd.npy', allow_pickle=True)\n",
        "w2v_product_embd = np.load(data_dir/'external'/'w2v_product_embd.npy', allow_pickle=True)\n",
        "image_item_embd = np.load(data_dir/'external'/'image_embd.npy', allow_pickle=True)\n",
        "w2v_sg_user_embd = np.load(data_dir/'external'/'w2v_skipgram_user_embd.npy', allow_pickle=True)\n",
        "w2v_sg_item_embd = np.load(data_dir/'external'/'w2v_skipgram_item_embd.npy', allow_pickle=True)\n",
        "w2v_sg_product_embd = np.load(data_dir/'external'/'w2v_skipgram_product_embd.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "M32FeEH_PS1w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiszC35_47or"
      },
      "source": [
        "## Load Candidates & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7KbLtnk4z0z",
        "outputId": "e3e475d9-e8d5-441d-b6ca-8c8edd13dfb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:11<00:45, 11.46s/it]"
          ]
        }
      ],
      "source": [
        "candidates = {}\n",
        "labels = {}\n",
        "for i in tqdm(range(1, WEEK_NUM)):\n",
        "    candidates[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n",
        "    candidates[i] = candidates[i][candidates[i]['rank']<=80]\n",
        "    labels[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_label.pqt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkNhkfuq5EPw"
      },
      "outputs": [],
      "source": [
        "feats = [\n",
        "    x\n",
        "    for x in candidates[1].columns\n",
        "    if x\n",
        "    not in [\n",
        "        \"label\",\n",
        "        \"sales_channel_id\",\n",
        "        \"t_dat\",\n",
        "        \"week\",\n",
        "\n",
        "        # 'i_w_full_sale_ratio',\n",
        "        # 'i_2w_full_sale_ratio',\n",
        "        'p_w_full_sale_ratio',\n",
        "        'p_2w_full_sale_ratio',\n",
        "        # 'i_week_above_daily_sale',\n",
        "        'p_week_above_full_sale',\n",
        "        # 'i_2w_week_above_daily_sale',\n",
        "        'p_2w_week_above_daily_sale',\n",
        "        'product_type_no_daily_sale',\n",
        "        # 'i_product_type_no_daily_sale_ratio',\n",
        "        'p_product_type_no_daily_sale_ratio',\n",
        "\n",
        "        # 'i_3w_sale',\n",
        "        # 'i_3w_sale_rank',\n",
        "        # 'i_3w_sale_norm',\n",
        "        'p_3w_sale',\n",
        "        'p_3w_sale_rank',\n",
        "        'p_3w_sale_norm',\n",
        "        # 'i_4w_sale',\n",
        "        # 'i_4w_sale_rank',\n",
        "        # 'i_4w_sale_norm',\n",
        "        'p_4w_sale',\n",
        "        'p_4w_sale_rank',\n",
        "        'p_4w_sale_norm',\n",
        "        # \"rank\",\n",
        "        # \"score\",\n",
        "        # \"prob\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "ids = [\"customer_id\", \"article_id\", \"product_code\"]\n",
        "dense_feats = [x for x in feats if x not in ids]\n",
        "# feats = ids + cat_features + dense_feats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for f in tqdm(dense_feats):\n",
        "#     for i in range(1,WEEK_NUM):\n",
        "#         if f in candidates[i].columns:\n",
        "#             candidates[i][f] = candidates[i][f].astype('float16')"
      ],
      "metadata": {
        "id": "dIfny2Isklqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYWW99nD556M"
      },
      "outputs": [],
      "source": [
        "full_data = pd.concat([candidates[i] for i in range(1,WEEK_NUM)], ignore_index=True)\n",
        "full_data = full_data[feats+['week','label']]\n",
        "gc.collect()\n",
        "# for f in tqdm(rule_feats):\n",
        "#     full_data[f] = full_data.groupby(['week','customer_id'])[f].rank()\n",
        "train = full_data[full_data['week']>1]\n",
        "valid = full_data[full_data['week']==1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del candidates\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "pVcq5-t7zreT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRJNcFdr9-45"
      },
      "outputs": [],
      "source": [
        "# Standardize\n",
        "# for feat in dense_feats:\n",
        "    # mask = train[feat].notnull()\n",
        "    # value = train.loc[mask, feat].mean()\n",
        "    # train[feat] = train[feat].fillna(value)\n",
        "    # valid[feat] = valid[feat].fillna(value)\n",
        "    # scaler = MinMaxScaler().fit(train[feat].values.reshape(-1,1))\n",
        "    # train[feat] = scaler.transform(train[feat].values.reshape(-1,1))\n",
        "    # valid[feat] = scaler.transform(valid[feat].values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8-Ohirc6SWH"
      },
      "outputs": [],
      "source": [
        "feat_dim = {}\n",
        "for feat in ids:\n",
        "    if feat in data['user'].columns:\n",
        "        feat_dim[feat] = int(data['user'][feat].max()) + 1\n",
        "    elif feat in data['item'].columns:\n",
        "        feat_dim[feat] = int(data['item'][feat].max()) + 1\n",
        "    else:\n",
        "        feat_dim[feat] = int(full_data[feat].max()) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9cISY640Y8x"
      },
      "outputs": [],
      "source": [
        "del full_data\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1 = train[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
        "X_train2 = np.zeros((X_train1.shape[0], len(dense_feats)), dtype='float32')\n",
        "for i,f in tqdm(enumerate(dense_feats)):\n",
        "    X_train2[:, i] = np.nan_to_num(train[f].values).astype('float32')\n",
        "    del train[f]\n",
        "y_train = train['label'].values"
      ],
      "metadata": {
        "id": "AqVs0eQC0FRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = valid[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
        "X_test2 = np.zeros((X_test1.shape[0], len(dense_feats)), dtype='float32')\n",
        "for i,f in tqdm(enumerate(dense_feats)):\n",
        "    X_test2[:, i] = np.nan_to_num(valid[f].values).astype('float32')\n",
        "    del valid[f]\n",
        "y_test = valid['label'].values"
      ],
      "metadata": {
        "id": "54XxZ1OP0Scb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxUVZlxJ0Y8y"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_embd_layer_1 = Embedding(\n",
        "    feat_dim[\"customer_id\"], 64, weights=[w2v_sg_user_embd], trainable=False\n",
        ")\n",
        "customer_embd_layer_2 = Embedding(\n",
        "    feat_dim[\"customer_id\"], 64, weights=[w2v_user_embd], trainable=False\n",
        ")\n",
        "customer_embd_layer_3 = Embedding(\n",
        "    feat_dim[\"customer_id\"], 128, weights=[tfidf_user], trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "B1ZH721w5pSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_embd_layer_1 = Embedding(\n",
        "    feat_dim[\"article_id\"], 64, weights=[w2v_sg_item_embd], trainable=False\n",
        ")\n",
        "\n",
        "article_embd_layer_2 = Embedding(\n",
        "    feat_dim[\"article_id\"], 64, weights=[w2v_item_embd], trainable=False\n",
        ")\n",
        "\n",
        "article_embd_layer_3 = Embedding(\n",
        "    feat_dim[\"article_id\"], 256, weights=[tfidf_item], trainable=False\n",
        ")\n",
        "\n",
        "article_embd_layer_4 = Embedding(\n",
        "    feat_dim[\"article_id\"], 128, weights=[tfidf_item2], trainable=False\n",
        ")\n",
        "\n",
        "article_embd_layer_5 = Embedding(\n",
        "    feat_dim[\"article_id\"], 512, weights=[image_item_embd], trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "1S2eMLNp5H5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_embd_layer_1 = Embedding(\n",
        "    feat_dim[\"product_code\"], 64, weights=[w2v_sg_product_embd], trainable=False\n",
        ")\n",
        "product_embd_layer_2 = Embedding(\n",
        "    feat_dim[\"product_code\"], 64, weights=[w2v_product_embd], trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "bfru5_xrg8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = Input(shape=X_train1.shape[1:], dtype=tf.int64)\n",
        "inputs2 = Input(shape=X_train2.shape[1:], dtype=tf.float32)\n",
        "input1 = tf.cast(inputs1, dtype=tf.int64)\n",
        "\n",
        "x_c_id1 = customer_embd_layer_1(input1[:,0])\n",
        "x_c_id2 = customer_embd_layer_2(input1[:,0])\n",
        "x_c_id3 = customer_embd_layer_3(input1[:,0])\n",
        "\n",
        "x_a_id1 = article_embd_layer_1(input1[:,1])\n",
        "x_a_id2 = article_embd_layer_2(input1[:,1])\n",
        "x_a_id3 = article_embd_layer_3(input1[:,1])\n",
        "x_a_id3 = Dense(128)(x_a_id3)\n",
        "x_a_id4 = article_embd_layer_4(input1[:,1])\n",
        "x_a_id5 = article_embd_layer_5(input1[:,1])\n",
        "x_a_id5 = Dense(128)(x_a_id5)\n",
        "\n",
        "x_p_id1 = product_embd_layer_1(input1[:,2])\n",
        "x_p_id2 = product_embd_layer_2(input1[:,2])\n",
        "\n",
        "\n",
        "x_id = Concatenate(axis=-1)([\n",
        "    x_c_id1, x_c_id2,\n",
        "    x_a_id1, x_a_id2, x_a_id3, x_a_id4, x_a_id5,\n",
        "    x_p_id1, x_p_id2,\n",
        "])\n",
        "\n",
        "x0 = Concatenate(axis=-1)([x_id, BatchNormalization()(inputs2)])\n",
        "# x = Dropout(0.2)(x0)\n",
        "# x = Dense(1024, activation='swish')(x)\n",
        "x = Dropout(0.2)(x0)\n",
        "x = Dense(512, activation='swish')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "\n",
        "x = Concatenate(axis=-1)([x, x0])\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
        "model.summary()\n",
        "    \n",
        "model.compile(\n",
        "    tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "psg5LLL94URR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, mode='max')\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_dir/'large_model_nn.h5',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_auc',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train1, X_train2], y_train.astype(int), \n",
        "    shuffle=True,\n",
        "    batch_size=2048,\n",
        "    validation_data=([X_test1, X_test2], y_test.astype(int)),\n",
        "    epochs=30,\n",
        "    callbacks=[checkpoint, early_stop]\n",
        ")\n",
        "\n",
        "# 0.7565"
      ],
      "metadata": {
        "id": "K6Um4beR62FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_dir/'large_model_nn.h5')"
      ],
      "metadata": {
        "id": "QGW114IL7I9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXOcPpTD9S0z"
      },
      "outputs": [],
      "source": [
        "probs = model.predict([X_test1, X_test2], batch_size=4096)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3qRJElkI2Jk"
      },
      "outputs": [],
      "source": [
        "label = data['inter'][data['inter']['t_dat']>='2020-09-16']\n",
        "label = label.groupby('customer_id')['article_id'].apply(list).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POdGXQWpJicG"
      },
      "outputs": [],
      "source": [
        "valid['prob'] = probs\n",
        "pred = valid.sort_values(by='prob',ascending=False).reset_index(drop=True)\n",
        "pred = pred.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
        "pred.columns = ['customer_id','prediction']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid = valid[['customer_id','article_id','prob']]"
      ],
      "metadata": {
        "id": "EMl0xTnfc2D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid.to_parquet(data_dir/'external'/'large_nn_valid.pqt')"
      ],
      "metadata": {
        "id": "xFYhQSVPc8nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWxE9Yp7JvRa"
      },
      "outputs": [],
      "source": [
        "label = label.merge(pred, on='customer_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfQX9y5iJy6x"
      },
      "outputs": [],
      "source": [
        "map_at_k(label['article_id'], label['prediction'], k=12)\n",
        "# 0.028500554033301987\n",
        "# 0.029904528760153\n",
        "\n",
        "# 0.031648009478868075\n",
        "# 0.031309369857160076\n",
        "\n",
        "# 031769005497044554"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "Bk7Z1JIsSkFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_dir/'large_model_nn.h5')"
      ],
      "metadata": {
        "id": "PRkjESDvSlrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TQDMPredictCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, custom_tqdm_instance=None, tqdm_cls=tqdm, **tqdm_params):\n",
        "        super().__init__()\n",
        "        self.tqdm_cls = tqdm_cls\n",
        "        self.tqdm_progress = None\n",
        "        self.prev_predict_batch = None\n",
        "        self.custom_tqdm_instance = custom_tqdm_instance\n",
        "        self.tqdm_params = tqdm_params\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        self.tqdm_progress.update(batch - self.prev_predict_batch)\n",
        "        self.prev_predict_batch = batch\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        self.prev_predict_batch = 0\n",
        "        if self.custom_tqdm_instance:\n",
        "            self.tqdm_progress = self.custom_tqdm_instance\n",
        "            return\n",
        "\n",
        "        total = self.params.get('steps')\n",
        "        if total:\n",
        "            total -= 1\n",
        "\n",
        "        self.tqdm_progress = self.tqdm_cls(total=total, **self.tqdm_params)\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        if self.tqdm_progress and not self.custom_tqdm_instance:\n",
        "            self.tqdm_progress.close()"
      ],
      "metadata": {
        "id": "aMSekWFSRO11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train, valid, X_train1, X_train2, X_test1, X_test2\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "lpTOgjxSSqRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = 0"
      ],
      "metadata": {
        "id": "uYGqgcXb0ZWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n",
        "for f in tqdm(dense_feats):\n",
        "    test_candidates[f] = test_candidates[f].astype('float16')\n",
        "test1 = test_candidates[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
        "test2 = np.zeros((test1.shape[0], len(dense_feats)), dtype='float32')\n",
        "for i,f in tqdm(enumerate(dense_feats)):\n",
        "    test2[:, i] = np.nan_to_num(test_candidates[f].values).astype('float32')\n",
        "    del test_candidates[f]\n",
        "gc.collect()\n",
        "\n",
        "probs = model.predict([test1, test2], batch_size=2048, callbacks=[TQDMPredictCallback()])\n",
        "test_candidates[\"prob\"] = probs\n",
        "pred_lgb = test_candidates[['customer_id','article_id','prob']]\n",
        "pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
        "pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n",
        "pred_lgb.to_parquet(data_dir/'interim'/f'large_nn_test_{chunk}.pqt')"
      ],
      "metadata": {
        "id": "1Dd8Mk_UdfcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = 1"
      ],
      "metadata": {
        "id": "AuqjTH6_PKbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n",
        "for f in tqdm(dense_feats):\n",
        "    test_candidates[f] = test_candidates[f].astype('float16')\n",
        "test1 = test_candidates[['customer_id', 'article_id', 'product_code']].values.astype('int32')\n",
        "test2 = np.zeros((test1.shape[0], len(dense_feats)), dtype='float32')\n",
        "for i,f in tqdm(enumerate(dense_feats)):\n",
        "    test2[:, i] = np.nan_to_num(test_candidates[f].values).astype('float32')\n",
        "    del test_candidates[f]\n",
        "gc.collect()\n",
        "\n",
        "probs = model.predict([test1, test2], batch_size=2048, callbacks=[TQDMPredictCallback()])\n",
        "test_candidates[\"prob\"] = probs\n",
        "pred_lgb = test_candidates[['customer_id','article_id','prob']]\n",
        "pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n",
        "pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n",
        "pred_lgb.to_parquet(data_dir/'interim'/f'large_nn_test_{chunk}.pqt')"
      ],
      "metadata": {
        "id": "JuZYoewEPLzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ivhW4xQuPRXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred1 = pd.read_parquet(data_dir/'interim'/f'nn_test_0.pqt')\n",
        "test_pred2 = pd.read_parquet(data_dir/'interim'/f'nn_test_1.pqt')"
      ],
      "metadata": {
        "id": "83ftWTZN5z95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = pd.concat([test_pred1, test_pred2], ignore_index=True)\n",
        "test_pred = test_pred.sort_values(by=[\"prob\"], ascending=False).reset_index(drop=True)\n",
        "test_pred = test_pred.drop_duplicates(['customer_id', 'prediction'], keep='first')"
      ],
      "metadata": {
        "id": "MgEvjLJvYFry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred.to_parquet(data_dir/'processed'/'nn_test.pqt')"
      ],
      "metadata": {
        "id": "mjx-3arOaAx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DNN LargeRecall.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
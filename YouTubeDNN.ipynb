{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9UxNHuPMuW2"
      },
      "source": [
        "# 导入需要的库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "C_ZR6gzp1E2N",
        "outputId": "8401132a-5090-464a-879d-a27492416f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:root:\n",
            "DeepCTR version 0.9.0 detected. Your version is 0.8.2.\n",
            "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.9.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
        "from preprocess import gen_data_set, gen_model_input\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "from deepmatch.models import *\n",
        "from deepmatch.utils import sampledsoftmaxloss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQq6O9XAMzPF"
      },
      "source": [
        "# 读取数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "article = pd.read_parquet('./data/articles.pqt')\n",
        "customer = pd.read_parquet('./data/customers.pqt')\n",
        "trans = pd.read_parquet('./data/transactions.pqt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2index = pickle.load(open('./data/id2index.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer[['FN','Active']] = customer[['FN','Active']].fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer['customer_id'] = customer['customer_id'].map(id2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = pd.merge(trans, customer, on=['customer_id'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_feats = ['article_id', 'product_code', 'product_type_no', 'product_group_name', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']\n",
        "trans = pd.merge(trans, article[article_feats], on='article_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans.to_parquet('./data/merge_v1.pqt', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "lcO29zFb21Od",
        "outputId": "97d9023e-cbd5-43dd-d2ef-769e3a34cf2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './ml-1m/users.dat'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-a76b4dcea94c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0munames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'occupation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'zip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'ml-1m/users.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'::'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'movie_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'ml-1m/ratings.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'::'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"readline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\coding\\Anaconda\\envs\\tf1.14\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ml-1m/users.dat'"
          ]
        }
      ],
      "source": [
        "data_path = \"./\"\n",
        "\n",
        "unames = ['user_id','gender','age','occupation','zip']\n",
        "user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
        "rnames = ['user_id','movie_id','rating','timestamp']\n",
        "ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
        "mnames = ['movie_id','title','genres']\n",
        "movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames)\n",
        "\n",
        "data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:10000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yCWxQxM3se"
      },
      "source": [
        "# 构建特征列，训练模型，导出embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BMOvk_de2ML3",
        "outputId": "1e43a5e7-f71c-45a4-bab4-e1d6b1a73669"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6040/6040 [00:12<00:00, 489.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 6\n",
            "WARNING:tensorflow:Entity <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7eff6fa7b358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7eff6fa7b358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7eff6fa7b358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7eff6fa7b358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepctr/layers/utils.py:167: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepctr/layers/utils.py:193: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7eff6fa8f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7eff6fa8f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7eff6fa8f128>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7eff6fa8f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7eff6fa8f128>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x7eff6fa92c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x7eff6fa92c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x7eff6fa92c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x7eff6fa92c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Epoch 1/24\n",
            "988129/988129 [==============================] - 22s 23us/sample - loss: 4.0715\n",
            "Epoch 2/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 3.7855\n",
            "Epoch 3/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.5596\n",
            "Epoch 4/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 3.4199\n",
            "Epoch 5/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.3216\n",
            "Epoch 6/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 3.2499\n",
            "Epoch 7/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.1846\n",
            "Epoch 8/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.1264\n",
            "Epoch 9/24\n",
            "988129/988129 [==============================] - 22s 22us/sample - loss: 3.0923\n",
            "Epoch 10/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.0566\n",
            "Epoch 11/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.0336\n",
            "Epoch 12/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 3.0066\n",
            "Epoch 13/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9872\n",
            "Epoch 14/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9659\n",
            "Epoch 15/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9476\n",
            "Epoch 16/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9363\n",
            "Epoch 17/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 2.9267\n",
            "Epoch 18/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9179\n",
            "Epoch 19/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.9012\n",
            "Epoch 20/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 2.8925\n",
            "Epoch 21/24\n",
            "988129/988129 [==============================] - 21s 21us/sample - loss: 2.8830\n",
            "Epoch 22/24\n",
            "988129/988129 [==============================] - 22s 22us/sample - loss: 2.8797\n",
            "Epoch 23/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.8711\n",
            "Epoch 24/24\n",
            "988129/988129 [==============================] - 21s 22us/sample - loss: 2.8619\n",
            "(6040, 32)\n",
            "(3706, 32)\n"
          ]
        }
      ],
      "source": [
        "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
        "sparse_features = [\"movie_id\", \"user_id\",\n",
        "                    \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
        "SEQ_LEN = 50\n",
        "negsample = 0\n",
        "\n",
        "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
        "\n",
        "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip']\n",
        "feature_max_idx = {}\n",
        "for feature in features:\n",
        "    lbe = LabelEncoder()\n",
        "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
        "    feature_max_idx[feature] = data[feature].max() + 1\n",
        "\n",
        "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
        "\n",
        "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
        "\n",
        "user_profile.set_index(\"user_id\", inplace=True)\n",
        "\n",
        "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
        "\n",
        "train_set, test_set = gen_data_set(data, negsample)\n",
        "\n",
        "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
        "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
        "\n",
        "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
        "\n",
        "embedding_dim = 32\n",
        "\n",
        "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
        "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
        "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
        "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
        "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
        "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
        "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
        "                        ]\n",
        "\n",
        "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
        "\n",
        "# 3.Define Model and train\n",
        "\n",
        "K.set_learning_phase(True)\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.__version__ >= '2.0.0':\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=100, user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,p=1,k_max=2,num_sampled=100,user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
        "\n",
        "history = model.fit(train_model_input, train_label,  # train_label,\n",
        "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, )\n",
        "\n",
        "# 4. Generate user features for testing and full item features for retrieval\n",
        "test_user_model_input = test_model_input\n",
        "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
        "\n",
        "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
        "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
        "\n",
        "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
        "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
        "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
        "\n",
        "print(user_embs.shape)\n",
        "print(item_embs.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_G3KWslKmJo"
      },
      "source": [
        "# 使用faiss进行ANN查找并评估结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SvyQLNVKkcs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "j2ZNYNBOOqrN",
        "outputId": "2938673c-ff81-49a2-86d8-4266d2060ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.6/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss-cpu) (1.18.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "6TY1l27iJU8U",
        "outputId": "5316c37f-fef1-44b3-8c31-6600d1e44da5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6040it [00:01, 4290.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "recall 0.26473509933774836\n",
            "hit rate 0.26473509933774836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from deepmatch.utils import recall_N\n",
        "\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "# faiss.normalize_L2(item_embs)\n",
        "index.add(item_embs)\n",
        "# faiss.normalize_L2(user_embs)\n",
        "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
        "s = []\n",
        "hit = 0\n",
        "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
        "    try:\n",
        "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
        "        filter_item = None\n",
        "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
        "        s.append(recall_score)\n",
        "        if test_true_label[uid] in pred:\n",
        "            hit += 1\n",
        "    except:\n",
        "        print(i)\n",
        "print(\"\")\n",
        "print(\"recall\", np.mean(s))\n",
        "print(\"hit rate\", hit / len(test_user_model_input['user_id']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a97TB0obOrRe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "colab_MovieLen1M_YoutubeDNN",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
